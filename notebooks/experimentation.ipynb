{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'toml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_objs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtoml\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# from xgboost import XGBClassifier\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'toml'"
     ]
    }
   ],
   "source": [
    "import flash as fz\n",
    "import joblib\n",
    "import klib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import plotly.graph_objs as go\n",
    "import toml\n",
    "# from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like people with a co-applicant income of 0 doesn't have a co-applicant. So, we should create a new feature called 'has_coapplicant'. For this feature, set the value to 'no' for individuals with a co-applicant income of 0, and 'yes' for those with a non-zero co-applicant income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_coapplicant'] = np.where(df['coapplicant_income'] == 0, 'no', 'yes')\n",
    "\n",
    "# Test\n",
    "df['has_coapplicant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending newly created features based on their feature type\n",
    "cat_cols.append('has_coapplicant')\n",
    "\n",
    "# Test\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical measures\n",
    "df[['has_coapplicant']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot\n",
    "sns.countplot(x=df['has_coapplicant'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "fz.crosstab_heatmap_viz(df, cat_cols, ['has_coapplicant'], 'both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point plot\n",
    "fig, axs = fz.num_cat_viz(df, num_cols, 'has_coapplicant', kind='point')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "fz.crosstab_heatmap_viz(df, ['loan_status'], ['has_coapplicant'], 'both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = fz.feature_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applicant_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "fig, axs = fz.feature_transform_viz(df['applicant_income'], transformed_data)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ Plot\n",
    "fig, axs = fz.feature_transform_viz(df['applicant_income'], transformed_data, kind='qq')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coapplicant_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "fig, axs = fz.feature_transform_viz(df['coapplicant_income'], transformed_data)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ Plot\n",
    "fig, axs = fz.feature_transform_viz(df['coapplicant_income'], transformed_data, kind='qq')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loan_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "fig, axs = fz.feature_transform_viz(df['loan_amount'], transformed_data)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ Plot\n",
    "fig, axs = fz.feature_transform_viz(df['loan_amount'], transformed_data, kind='qq')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- **applicant_income & loan_amount:** Quantile Transform normalizes the data effectively.  \n",
    "- **coapplicant_income:** Reciprocal Transform transforms coapplicant_income to follow a bimodal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['applicant_income'] = transformed_data['Quantile']['applicant_income']\n",
    "df['coapplicant_income'] = transformed_data['Reciprocal']['coapplicant_income']\n",
    "df['loan_amount'] = transformed_data['Quantile']['loan_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "fig, axs = fz.hist_box_viz(df[num_cols])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/raw/loan_sanction_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Understand structure of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless features\n",
    "df.drop(\"Loan_ID\", axis=1, inplace=True)\n",
    "\n",
    "# Test\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "df = klib.clean_column_names(df)\n",
    "\n",
    "# Test\n",
    "column_names = df.columns.tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate data points\n",
    "def check_duplicates(df):\n",
    "    if df.duplicated().any():\n",
    "        print(df[df.duplicated(keep=False)])\n",
    "    else:\n",
    "        print(\"There are no duplicate data points in the dataframe\")\n",
    "\n",
    "\n",
    "check_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful information that we can get from df.info():\n",
    "\n",
    "- Feature names\n",
    "- Number of data points\n",
    "- Number of features\n",
    "- Data type of features\n",
    "- Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numerical, categorical, and other features from the dataset\n",
    "num_cols, cat_cols, other_cols = fz.extract_features(\n",
    "    df, \"all\", ignore_cols=[\"loan_status\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print numerical features of dataset\n",
    "df[num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print categorical features of dataset\n",
    "df[cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "target_col = [\"loan_status\"]\n",
    "df = df[num_cols + cat_cols + target_col]\n",
    "\n",
    "# Test\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical measures\n",
    "df[num_cols].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram & Box-plot\n",
    "fig, axs = fz.hist_box_viz(df[num_cols])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features\n",
    "num_nan_pct = fz.calc_nan_values(df[num_cols])\n",
    "num_cols_with_nan = num_nan_pct.index.tolist()\n",
    "\n",
    "print(num_nan_pct)  # Percentage of missing values in numerical features\n",
    "print(num_cols_with_nan)  # Numerical features with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "cat_nan_pct = fz.calc_nan_values(df[cat_cols])\n",
    "cat_cols_with_nan = cat_nan_pct.index.tolist()\n",
    "\n",
    "print(cat_nan_pct)  # Percentage of missing values in categorical features\n",
    "print(cat_cols_with_nan)  # Categorical features with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the target column contains any missing values\n",
    "df[\"loan_status\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of missing values to determine the type of missing values\n",
    "fig, axs = fz.nan_value_viz(df[num_cols_with_nan + cat_cols_with_nan])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and target\n",
    "X = df.drop('loan_status', axis=1)\n",
    "y = df['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'applicant_income'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/machine-learning/loan-sanction-prediction/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'applicant_income'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mapplicant_income\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mapplicant_income\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.astype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m      2\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mloan_amount_term\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mloan_amount_term\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m).astype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m      3\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcredit_history\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mcredit_history\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m).astype(\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/machine-learning/loan-sanction-prediction/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/machine-learning/loan-sanction-prediction/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'applicant_income'"
     ]
    }
   ],
   "source": [
    "df['applicant_income'] = df['applicant_income'].astype(float)\n",
    "df['loan_amount_term'] = df['loan_amount_term'].astype(int).astype(object)\n",
    "df['credit_history'] = df['credit_history'].astype(int).astype(object)\n",
    "\n",
    "# Test\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export config data\n",
    "\n",
    "config_data = {\n",
    "    \"column_names\": column_names,\n",
    "    \"num\": {\"cols\": num_cols, \"nan\": num_cols_with_nan},\n",
    "    \"cat\": {\"cols\": cat_cols, \"nan\": cat_cols_with_nan},\n",
    "    \"target_col\": target_col,\n",
    "}\n",
    "\n",
    "with open(\"../src/loan_sanction_prediction/config.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_data, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to ../data/interim/cleaned_train_data_v1.csv\n"
     ]
    }
   ],
   "source": [
    "# Export dataset\n",
    "fz.export(df, \"../data/interim/cleaned_train_data_v1.csv\", force_overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Handle missing values in categorical columns\n",
    "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")\n",
    "X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder expects datatypes of every value in a column to be the same\n",
    "X[cat_cols] = X[cat_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store encoded feature names before encoding\n",
    "unique_values_in_cols = {}\n",
    "for col in cat_cols:\n",
    "    encoded_columns = []\n",
    "    for value in X[col].unique():\n",
    "        encoded_columns.append(f\"{col}_{value}\")\n",
    "    unique_values_in_cols[col] = encoded_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. One-Hot Encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "encoded_X_data = encoder.fit_transform(X[cat_cols])\n",
    "encoded_X_df = pd.DataFrame(encoded_X_data, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# Concatenating encoded categorical features with the rest of the X\n",
    "X = pd.concat([X.drop(columns=cat_cols), encoded_X_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Impute missing values in numerical features using KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X = pd.DataFrame(knn_imputer.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Assign the missing value imputed numerical features back to df\n",
    "df[num_cols_with_nan] = X[num_cols_with_nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_categorical_imputer(X, y, clf_model):\n",
    "    X, y = X.copy(), y.copy() # Avoid modifying the original X and y\n",
    "\n",
    "    y_notna = y.notna() # Create a mask for non-missing values in y\n",
    "\n",
    "    # Split the data into training (non-missing) and test (missing) data\n",
    "    X, X_test = X[y_notna], X[~y_notna]\n",
    "    y_train, y_test = y[y_notna], y[~y_notna]\n",
    "\n",
    "    # Label encoding the target feature\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    clf_model.fit(X, y_train) # Train the model\n",
    "\n",
    "    y_pred = clf_model.predict(X_test) # Predict on the test data (missing values)\n",
    "\n",
    "    # Inverse transform the predicted values to original labels\n",
    "    y_pred_inverse = le.inverse_transform(y_pred)\n",
    "\n",
    "    y[y_test.index] = y_pred_inverse # Impute the missing target values\n",
    "\n",
    "    return y, clf_model, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_models = {}\n",
    "label_encoders = {}\n",
    "for col in cat_cols_with_nan:\n",
    "    df[col], clf_models[col], label_encoders[col] = advanced_categorical_imputer(\n",
    "        X.drop(columns=unique_values_in_cols[col], errors='ignore'), df[col],\n",
    "        ExtraTreesClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    if df.isna().any().any():\n",
    "        print(\"There are still missing values in the DataFrame.\")\n",
    "    else:\n",
    "        print(\"There are no missing values left in the DataFrame.\")\n",
    "\n",
    "# Test\n",
    "check_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configurations\n",
    "with open(\"config/config_v2.toml\", \"r\") as file:\n",
    "    config_data = toml.load(file)\n",
    "\n",
    "num_cols, cat_cols = config_data['num']['cols'], config_data['cat']['cols']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/interim/feature_engineered_train_data_v1.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('loan_status', axis=1)\n",
    "y = df['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranformer for preprocessing data\n",
    "transformer = make_column_transformer(\n",
    "    (StandardScaler(), num_cols),\n",
    "    (OneHotEncoder(drop='first', sparse_output=False), cat_cols),\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = transformer.fit_transform(X)\n",
    "\n",
    "# Test\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the dataset using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_transformed, y_resampled = smote.fit_resample(X_transformed, y)\n",
    "\n",
    "# Test\n",
    "print(X_transformed.shape, y_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "unique_values, counts = np.unique(y_resampled, return_counts=True)\n",
    "\n",
    "# Print the counts of each class\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"Class {value}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection (Before hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Decision Trees': DecisionTreeClassifier(),\n",
    "    'Xgboost': XGBClassifier(),\n",
    "    'Extra Trees': ExtraTreesClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric functions\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score,\n",
    "    'precision': precision_score,\n",
    "    'recall': recall_score,\n",
    "    'f1': f1_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models_across_metrics(models, metrics, X, y, cv=5):\n",
    "    models_across_metrics = {metric: {} for metric in metrics}\n",
    "    for metric in metrics:\n",
    "        for model_name, model in models.items():\n",
    "            cv_scores = cross_val_score(model, X, y, cv=cv, scoring=metric)\n",
    "            cv_scores_mean = cv_scores.mean()\n",
    "            models_across_metrics[metric][model_name] = round(cv_scores_mean, 3)\n",
    "    return pd.DataFrame(models_across_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_across_metrics = eval_models_across_metrics(models, metrics.keys(), X_transformed, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_across_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- After evaluating the metrics, I have decided to focus on the top 3 models (in terms of accuracy_score): Random Forest Classifier, Extra Trees Classifier, Xgboost Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top models for further hyperparameter tuning\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Xgboost': XGBClassifier(),\n",
    "    'Extra Trees': ExtraTreesClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 200],\n",
    "        'max_depth': [None, 30],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 4]\n",
    "    },\n",
    "    'Xgboost': {\n",
    "        'n_estimators': [50, 200],\n",
    "        'max_depth': [3, 10],\n",
    "        'learning_rate': [0.01, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'gamma': [0, 0.2]\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'n_estimators': [50, 200],\n",
    "        'max_depth': [None, 30],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(models, param_grids, X, y):\n",
    "    best_params = {}\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Processing {model_name}...\")\n",
    "        param_grid = param_grids[model_name]\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1,\n",
    "            verbose=1\n",
    "            )\n",
    "        grid_search.fit(X, y)\n",
    "        best_params[model_name] = {\n",
    "            'Best Parameters': grid_search.best_params_,\n",
    "            'Average accuracy score on the best parameters': round(grid_search.best_score_, 3)\n",
    "        }\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding best hyperparameters on top models using GridSearchCV\n",
    "best_params = perform_grid_search(models, param_grids, X_transformed, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params\n",
    "\n",
    "# {'Random Forest': {'Best Parameters': {'max_depth': None,\n",
    "#    'min_samples_leaf': 1,\n",
    "#    'min_samples_split': 2,\n",
    "#    'n_estimators': 200},\n",
    "#   'Average accuracy score on the best parameters': 0.838},\n",
    "#  'Xgboost': {'Best Parameters': {'colsample_bytree': 0.8,\n",
    "#    'gamma': 0.2,\n",
    "#    'learning_rate': 0.2,\n",
    "#    'max_depth': 10,\n",
    "#    'n_estimators': 50,\n",
    "#    'subsample': 1.0},\n",
    "#   'Average accuracy score on the best parameters': 0.826},\n",
    "#  'Extra Trees': {'Best Parameters': {'bootstrap': True,\n",
    "#    'max_depth': 30,\n",
    "#    'min_samples_leaf': 1,\n",
    "#    'min_samples_split': 2,\n",
    "#    'n_estimators': 200},\n",
    "#   'Average accuracy score on the best parameters': 0.84}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top models with best hyperparameters\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(**best_params['Random Forest']['Best Parameters']),\n",
    "    'Xgboost': XGBClassifier(**best_params['Xgboost']['Best Parameters']),\n",
    "    'Extra Trees': ExtraTreesClassifier(**best_params['Extra Trees']['Best Parameters'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing top models across metrics after hyperparameter tuning\n",
    "models_across_metrics = eval_models_across_metrics(models, metrics.keys(), X_transformed, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_across_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "for model_name, model in models.items():\n",
    "    estimators.append((model_name, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_voting_clf(estimators, X, y, cv = 5):\n",
    "    # Create a voting classifier (hard voting)\n",
    "    voting_clf_hard = VotingClassifier(estimators=estimators, voting='hard')\n",
    "\n",
    "    # Create a voting classifier (soft voting)\n",
    "    voting_clf_soft = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "    # Apply cross-validation\n",
    "    cv_scores_h = cross_val_score(voting_clf_hard, X, y, cv=cv, scoring='accuracy')\n",
    "    cv_scores_s = cross_val_score(voting_clf_soft, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "    accuracy_results = {}\n",
    "\n",
    "    accuracy_results['Hard Margin'] = round(cv_scores_h.mean(), 3)\n",
    "    accuracy_results['Soft Margin'] = round(cv_scores_s.mean(), 3)\n",
    "\n",
    "    return accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on hard and soft margin voting classifiers\n",
    "accuracy = eval_voting_clf(estimators, X_transformed, y_resampled)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best model\n",
    "voting_clf = VotingClassifier(estimators, voting='hard')\n",
    "voting_clf.fit(X_transformed, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),     # Step 1: Standardize the data\n",
    "    PCA(n_components=2),  # Step 2: Apply PCA\n",
    "    LogisticRegression()  # Step 3: Train a logistic regression model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "pipe = make_pipeline(\n",
    "    (transformer),\n",
    "    ('model', voting_clf)\n",
    ")\n",
    "\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Machine Learning model\n",
    "joblib.dump(voting_clf, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features' data type to category\n",
    "# This will be helpful while doing analysis\n",
    "df[cat_cols] = df[cat_cols].astype('category')\n",
    "\n",
    "# Test\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical measures\n",
    "df[num_cols].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical moments\n",
    "fz.stats_moments(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histogram & boxplot\n",
    "fig, axs = fz.hist_box_viz(df[num_cols])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "\n",
    "- There are many outliers on the upper side of all numerical features, while none are present on the lower side.\n",
    "- Since the outliers appear to be valid and are not due to data entry issues, we don't have to drop them.\n",
    "- None of the numerical features follow a normal distribution.\n",
    "- The distributions of applicant income and loan amount are right-skewed (positively skewed).\n",
    "- Feature transformation is required for all numerical features to address this skewness.\n",
    "- It looks like people with a co-applicant income of 0 doesn't have a co-applicant. So, we should create a new feature called 'has_coapplicant'. For this feature, set the value to 'no' for individuals with a co-applicant income of 0, and 'yes' for those with a non-zero co-applicant income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical measures\n",
    "df[cat_cols].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplots\n",
    "fig, axs = fz.count_viz(df[cat_cols])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "\n",
    "- A higher number of males apply for loans compared to females.\n",
    "- Married individuals are more likely to apply for loans than unmarried individuals, with approximately twice as many married applicants.\n",
    "- Individuals without dependents apply for loans more frequently than those with dependents.\n",
    "- Graduates are more likely to apply for loans than non-graduates.\n",
    "- Non-self-employed individuals apply for loans more than self-employed individuals.\n",
    "- People whose property is located in semi-urban areas tend to apply for loans more than those with properties in rural or urban areas. Those with property in rural areas apply for the fewest loans, although these trends are not very strong.\n",
    "- The majority of loan applicants prefer a loan term of 360 months (30 years), followed by 180 months (15 years). Other loan term durations are relatively rare.\n",
    "- Individuals with a credit history of 1 are more likely to apply for loans compared to those with a credit history of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical - Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "grid = fz.pair_viz(df[num_cols])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "methods=['pearson', 'spearman', 'kendall']\n",
    "for method in methods:\n",
    "    fz.corr_heatmap_viz(df[num_cols], method=method)\n",
    "    plt.show()\n",
    "    print(\"-\"*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- None of the features show a strong linear relationship with each other. However, there is a moderate relationship between applicant income and loan amount. This makes sense because individuals with higher incomes often need larger loan amounts.\n",
    "\n",
    "- Pearson, Spearman, and Kendall\n",
    "Tau's correlations show similar patterns, but their values are slightly different. Since the heatmaps from all of these are similar, the exact values are less important. In this case, Spearman's correlation is more suitable because the data isn't normally distributed, doesn't have a linear relationship between features, and has outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstab Heatmap\n",
    "fz.crosstab_heatmap_viz(df, cat_cols, normalize='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical - Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applicant_income\n",
    "fig, axs = fz.num_cat_viz(df, 'applicant_income', cat_cols)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coapplicant_income\n",
    "fig, axs = fz.num_cat_viz(df, 'coapplicant_income', cat_cols)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan_amount\n",
    "fig, axs = fz.num_cat_viz(df, 'loan_amount', cat_cols)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KDE-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applicant_income\n",
    "fig, axs = fz.num_cat_viz(df, 'applicant_income', cat_cols, kind='kde')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coapplicant_income\n",
    "fig, axs = fz.num_cat_viz(df, 'coapplicant_income', cat_cols, kind='kde')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan_amount\n",
    "fig, axs = fz.num_cat_viz(df, 'loan_amount', cat_cols, kind='kde')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applicant_income\n",
    "fig, axs = fz.num_cat_viz(df, 'applicant_income', cat_cols, kind='point')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coapplicant_income\n",
    "fig, axs = fz.num_cat_viz(df, 'coapplicant_income', cat_cols, kind='point')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan_amount\n",
    "fig, axs = fz.num_cat_viz(df, 'loan_amount', cat_cols, kind='point')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mulitvariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical - Numerical - Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter3d(\n",
    "    x=df['applicant_income'],\n",
    "    y=df['coapplicant_income'],\n",
    "    z=df['loan_amount'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5)\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Applicant Income',\n",
    "        yaxis_title='Coapplicant Income',\n",
    "        zaxis_title='Loan Amount'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical - Numerical - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "for feature in cat_cols:\n",
    "    sns.pairplot(df, vars=num_cols, hue=feature)\n",
    "    plt.show()\n",
    "    print(\"-\"*105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_num_cat_viz(x, y, categorical_features):\n",
    "    for feature in categorical_features:\n",
    "        sns.relplot(df, x=x, y=y, col=feature)\n",
    "        plt.show()\n",
    "        print(\"-\"*118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applicant_income & coapplicant_income\n",
    "num_num_cat_viz('applicant_income', 'coapplicant_income', cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applicant_income & loan_amount\n",
    "num_num_cat_viz('applicant_income', 'loan_amount', cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coapplicant_income & loan_amount\n",
    "num_num_cat_viz('coapplicant_income', 'loan_amount', cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical - Categorical - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df['loan_status'].value_counts(), labels=df['loan_status'].unique(), autopct='%0.2f%%',\n",
    "        shadow=True, explode=(0, 0.1), counterclock=False, colors=['lime', 'cyan'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "\n",
    "- The classes in target is moderately imbalanced. Need to handle the class imbalance using SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Categorical - Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstab Heatmap\n",
    "fz.crosstab_heatmap_viz(df, cat_cols, ['loan_status'], 'both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Numerical - Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-plot\n",
    "fig, axs = fz.num_cat_viz(df, num_cols, 'loan_status')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE-plot\n",
    "fig, axs = fz.num_cat_viz(df, num_cols, 'loan_status', kind='kde')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point-plot\n",
    "fig, axs = fz.num_cat_viz(df, num_cols, 'loan_status', kind='point')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mulitvariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Numerical - Numerical - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, vars = num_cols, hue='loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relplot(df, numerical_features, categorical_feature):\n",
    "    for i, feature_i in enumerate(numerical_features):\n",
    "        for j, feature_j in enumerate(numerical_features[i+1:], start=i+1):\n",
    "            sns.relplot(df, x=feature_i, y=feature_j, col=categorical_feature)\n",
    "            plt.show()\n",
    "            print(\"-\" * df[categorical_feature].nunique()*59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relplot(df, num_cols, 'loan_status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Numerical - Categorical - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
